{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STANDALONE BALANCE SHEET LINK FOR ALL COMPANIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing company: Gujarat Fluorochemicals\n",
      "Processing company: Gallantt Ispat Ltd\n",
      "Processing company: Rotomac Global Private Limited\n",
      "Processing company: Rotomac Exports Private Limited\n",
      "Processing company: Ankur Drugs and Pharma Limited\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to scrape Google search results\n",
    "def scrape_google(company_name):\n",
    "    search_query = f\"{company_name} Share/Stock Price Moneycontrol\"\n",
    "    url = f\"https://www.google.com/search?q={search_query}\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        search_results = soup.find_all(\"div\", class_=\"tF2Cxc\")\n",
    "        top_results = [result.a[\"href\"] for result in search_results]\n",
    "        return top_results[:5]  # Return top 5 URLs\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to extract financial links with specific keywords from a page\n",
    "def extract_links_with_keywords(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        financials_div = soup.find(\"div\", id=\"financials\")\n",
    "        mainprice_div = soup.find(\"div\", id=\"mainprice\")\n",
    "        scraped_company = \"\"  # New variable to store scraped company info\n",
    "        \n",
    "        if mainprice_div:\n",
    "            inid_name_div = mainprice_div.find(\"div\", class_=\"inid_name\", id=\"stockName\")\n",
    "            if inid_name_div:\n",
    "                h1_tag = inid_name_div.find(\"h1\")  # Find the <h1> tag\n",
    "                if h1_tag:\n",
    "                    scraped_company = h1_tag.text.strip()  # Get the text from the <h1> tag\n",
    "\n",
    "\n",
    "        if financials_div:\n",
    "            quick_links = financials_div.find(\"div\", class_=\"quick_links clearfix\")\n",
    "            if quick_links:\n",
    "                links = quick_links.find_all(\"a\")\n",
    "                extracted_links = {link.get(\"title\"): link.get(\"href\") for link in links\n",
    "                                    if any(keyword in link.text.lower() for keyword in [\"balance\"])}\n",
    "                company_name = extracted_links.get(\"Company Name\", \"\")  # Retrieve company name\n",
    "                \n",
    "                # Get the link inside the 'consolidated' id\n",
    "                if extracted_links:\n",
    "                    main_link = extracted_links.get(\"Balance Sheet\", \"\")  # Assuming 'Balance Sheet' holds the main link\n",
    "                    response = requests.get(main_link)\n",
    "                    if response.status_code == 200:\n",
    "                        soup_link = BeautifulSoup(response.text, \"html.parser\")\n",
    "                        consolidated_link = soup_link.find(\"div\", class_=\"subtabs nav-tabs\")\n",
    "                        if consolidated_link:\n",
    "                            consolidated_href = consolidated_link.find(\"li\", id=\"consolidated\").find(\"a\").get(\"href\", \"\")\n",
    "                            # Update the data dictionary with the consolidated link\n",
    "                            extracted_links[\"Consolidated Link\"] = consolidated_href\n",
    "                            print(consolidated_href)\n",
    "\n",
    "                # Create a dictionary with relevant data\n",
    "                data_dict = {\n",
    "                    \"Company Name\": company_name,\n",
    "                    \"Scraped Company\": scraped_company,\n",
    "                    \"Balance Sheet\": extracted_links.get(\"Balance Sheet\", \"\"),\n",
    "                    \"Consolidated Link\": extracted_links.get(\"Consolidated Link\", \"\")\n",
    "                }\n",
    "                return data_dict\n",
    "\n",
    "        # Return an empty dictionary if no data is fetched\n",
    "        return {}\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "google_sheets_link = \"https://docs.google.com/spreadsheets/d/1z2j632dxOP0qI8UFq8R9PMsyD0Slza1NJF0w3mMQYpQ/export?format=csv\"\n",
    "data = pd.read_csv(google_sheets_link)\n",
    "data = data.iloc[:5]\n",
    "\n",
    "# companies = data['Company Name']\n",
    "companies = []\n",
    "scraped_companies = []  # New list for scraped company names\n",
    "balance_sheet = []\n",
    "consol_balance_sheet = []\n",
    "\n",
    "# Iterate through each company name in the column \"Company Name\"\n",
    "for company_name in data[\"Company Name\"]:\n",
    "    print(f\"Processing company: {company_name}\")\n",
    "    google_results = scrape_google(company_name)\n",
    "\n",
    "    if google_results:\n",
    "        # Extract financial links with specific keywords from the top 5 search results\n",
    "        company_links = extract_links_with_keywords(google_results[0])  # Consider only the first result\n",
    "        \n",
    "        if company_links:\n",
    "            # Append data to respective lists\n",
    "            companies.append(company_name)\n",
    "            scraped_companies.append(company_links.get(\"Scraped Company\", \"\"))\n",
    "            balance_sheet.append(company_links.get(\"Balance Sheet\", \"\"))\n",
    "            consol_balance_sheet.append(company_links.get(\"Consolidated Link\", \"\"))\n",
    "            # ... (other columns follow the same pattern)\n",
    "        else:\n",
    "            companies.append(company_name)\n",
    "            scraped_companies.append(\"\")\n",
    "            balance_sheet.append(\"\")\n",
    "            consol_balance_sheet.append(\"\")\n",
    "\n",
    "# Create a DataFrame from the lists\n",
    "links_df = pd.DataFrame({\n",
    "    \"Company Name\": companies,\n",
    "    \"Scraped Company\": scraped_companies,\n",
    "    \"Balance Sheet\": balance_sheet,\n",
    "    \"Consolidated Balance Sheet\": consol_balance_sheet\n",
    "})\n",
    "\n",
    "# Save DataFrame to a CSV file\n",
    "links_df.to_csv(\"consolidated_financial_links.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing company: Paytm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to scrape Google search results\n",
    "def scrape_google(company_name):\n",
    "    search_query = f\"{company_name} Share/Stock Price Moneycontrol\"\n",
    "    url = f\"https://www.google.com/search?q={search_query}\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        search_results = soup.find_all(\"div\", class_=\"tF2Cxc\")\n",
    "        top_results = [result.a[\"href\"] for result in search_results]\n",
    "        return top_results[:5]  # Return top 5 URLs\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to extract financial links with specific keywords from a page\n",
    "def extract_links_with_keywords(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        financials_div = soup.find(\"div\", id=\"financials\")\n",
    "        mainprice_div = soup.find(\"div\", id=\"mainprice\")\n",
    "        scraped_company = \"\"  # New variable to store scraped company info\n",
    "        \n",
    "        if mainprice_div:\n",
    "            inid_name_div = mainprice_div.find(\"div\", class_=\"inid_name\", id=\"stockName\")\n",
    "            if inid_name_div:\n",
    "                h1_tag = inid_name_div.find(\"h1\")  # Find the <h1> tag\n",
    "                if h1_tag:\n",
    "                    scraped_company = h1_tag.text.strip()  # Get the text from the <h1> tag\n",
    "\n",
    "\n",
    "        if financials_div:\n",
    "            quick_links = financials_div.find(\"div\", class_=\"quick_links clearfix\")\n",
    "            if quick_links:\n",
    "                links = quick_links.find_all(\"a\")\n",
    "                extracted_links = {link.get(\"title\"): link.get(\"href\") for link in links\n",
    "                                    if any(keyword in link.text.lower() for keyword in [\"balance\"])}\n",
    "                company_name = extracted_links.get(\"Company Name\", \"\")  # Retrieve company name\n",
    "                \n",
    "                # Create a dictionary with relevant data\n",
    "                data_dict = {\n",
    "                    \"Company Name\": company_name,\n",
    "                    \"Scraped Company\": scraped_company,\n",
    "                    \"Balance Sheet\": extracted_links.get(\"Balance Sheet\", \"\")\n",
    "                }\n",
    "                return data_dict\n",
    "        None\n",
    "    else:\n",
    "        return {\n",
    "            None\n",
    "        }\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "google_sheets_link = \"https://docs.google.com/spreadsheets/d/1iJf5fGzSKJLkJkSRNB9i8QhlotFpP9hwRplyF9cX-jk/export?format=csv\"\n",
    "data = pd.read_csv(google_sheets_link)\n",
    "# data = data.iloc[:5]\n",
    "\n",
    "# companies = data['Company Name']\n",
    "companies = []\n",
    "scraped_companies = []  # New list for scraped company names\n",
    "balance_sheet = []\n",
    "\n",
    "# Iterate through each company name in the column \"Company Name\"\n",
    "for company_name in data[\"Company Name\"]:\n",
    "    print(f\"Processing company: {company_name}\")\n",
    "    google_results = scrape_google(company_name)\n",
    "\n",
    "    if google_results:\n",
    "        # Extract financial links with specific keywords from the top 5 search results\n",
    "        company_links = extract_links_with_keywords(google_results[0])  # Consider only the first result\n",
    "        \n",
    "        if company_links:\n",
    "            # Append data to respective lists\n",
    "            companies.append(company_name)\n",
    "            scraped_companies.append(company_links.get(\"Scraped Company\", \"\"))\n",
    "            balance_sheet.append(company_links.get(\"Balance Sheet\", \"\"))\n",
    "            # ... (other columns follow the same pattern)\n",
    "        else:\n",
    "            companies.append(company_name)\n",
    "            scraped_companies.append(\"\")\n",
    "            balance_sheet.append(\"\")\n",
    "\n",
    "# Create a DataFrame from the lists\n",
    "links_df = pd.DataFrame({\n",
    "    \"Company Name\": companies,\n",
    "    \"Scraped Company\": scraped_companies,\n",
    "    \"Balance Sheet\": balance_sheet\n",
    "})\n",
    "\n",
    "# Save DataFrame to a CSV file\n",
    "links_df.to_csv(\"CHECK_financial_links.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STANDALONE BALANCE SHEET FOR NEXT YEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Read the CSV file\n",
    "file_path = \"C:\\\\Users\\\\hp\\\\Downloads\\\\financialnumericaldatasetconnstructioncodefiles\\\\CHECK_financial_links.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Function to scrape CHECK_Consolidated links\n",
    "def scrape_CHECK_Consolidated_links(url):\n",
    "    CHECK_Consolidated_link = \"\"\n",
    "    if url and isinstance(url, str):\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            financial_section = soup.find(\"div\", class_=\"financial-section\")\n",
    "            if financial_section:\n",
    "                pagination = financial_section.find(\"ul\", class_=\"pagination clearfix\")\n",
    "                if pagination:\n",
    "                    list_items = pagination.find_all(\"li\")\n",
    "                    for item in list_items:\n",
    "                        nextpaging_span = item.find(\"span\", class_=\"nextpaging\")\n",
    "                        if nextpaging_span:\n",
    "                            link = item.find(\"a\")\n",
    "                            if link:\n",
    "                                CHECK_Consolidated_link = link.get(\"href\", \"\")\n",
    "                                break  # Stop after finding the desired link\n",
    "    return CHECK_Consolidated_link\n",
    "\n",
    "\n",
    "count = 0\n",
    "# Iterate through the \"Consolidated Link\" column\n",
    "CHECK_Consolidated_links = []\n",
    "for link in data[\"Balance Sheet\"]:\n",
    "    CHECK_Consolidated_links.append(scrape_CHECK_Consolidated_links(link) if pd.notnull(link) else \"\")\n",
    "    print(count)\n",
    "    count = count+1\n",
    "\n",
    "# Add the scraped links to the DataFrame\n",
    "data[\"Balance_Sheet_2\"] = CHECK_Consolidated_links\n",
    "\n",
    "# Save DataFrame back to CSV\n",
    "data.to_csv(\"CHECK_Standalone Link_2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GET CONSOLIDATED LINKS FROM STANDALONE LINKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Read the CSV file\n",
    "file_path = \"C:\\\\Users\\\\hp\\\\Downloads\\\\financialnumericaldatasetconnstructioncodefiles\\\\CHECK_financial_links.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Function to scrape consolidated links\n",
    "def scrape_consolidated_links(url):\n",
    "    consolidated_link = \"\"\n",
    "    if url and isinstance(url, str):\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            financial_section = soup.find(\"div\", class_=\"financial-section\")\n",
    "            if financial_section:\n",
    "                tab_content = financial_section.find(\"div\", class_=\"tab-content clearfix\")\n",
    "                if tab_content:\n",
    "                    nsebsetab = tab_content.find(\"div\", class_=\"nsebsetab subnsebsetab clearfix\")\n",
    "                    if nsebsetab:\n",
    "                        subtabs = nsebsetab.find(\"ul\", class_=\"subtabs nav-tabs\")\n",
    "                        if subtabs:\n",
    "                            consolidated_item = subtabs.find(\"a\", id=\"#consolidated\")\n",
    "                            if consolidated_item:\n",
    "                                consolidated_link = consolidated_item.get(\"href\", \"\")\n",
    "    return consolidated_link\n",
    "\n",
    "count = 0\n",
    "\n",
    "# Iterate through the \"Balance Sheet\" column\n",
    "consolidated_links = []\n",
    "for link in data[\"Balance Sheet\"]:\n",
    "    consolidated_links.append(scrape_consolidated_links(link) if pd.notnull(link) else \"\")\n",
    "    print(count)\n",
    "    count = count+1\n",
    "\n",
    "# Add the scraped links to the DataFrame\n",
    "data[\"CHECK_Consolidated Link\"] = consolidated_links\n",
    "\n",
    "# Save DataFrame back to CSV\n",
    "data.to_csv(\"CHECK_Consolidated.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONSOLIDATED FOR NEXT YEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Read the CSV file\n",
    "file_path = \"C:\\\\Users\\\\hp\\\\Downloads\\\\financialnumericaldatasetconnstructioncodefiles\\\\CHECK_Consolidated.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Function to scrape CHECK_Consolidated links\n",
    "def scrape_CHECK_Consolidated_links(url):\n",
    "    CHECK_Consolidated_link = \"\"\n",
    "    if url and isinstance(url, str):\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            financial_section = soup.find(\"div\", class_=\"financial-section\")\n",
    "            if financial_section:\n",
    "                pagination = financial_section.find(\"ul\", class_=\"pagination clearfix\")\n",
    "                if pagination:\n",
    "                    list_items = pagination.find_all(\"li\")\n",
    "                    for item in list_items:\n",
    "                        nextpaging_span = item.find(\"span\", class_=\"nextpaging\")\n",
    "                        if nextpaging_span:\n",
    "                            link = item.find(\"a\")\n",
    "                            if link:\n",
    "                                CHECK_Consolidated_link = link.get(\"href\", \"\")\n",
    "                                break  # Stop after finding the desired link\n",
    "    return CHECK_Consolidated_link\n",
    "\n",
    "\n",
    "count = 0\n",
    "# Iterate through the \"Consolidated Link\" column\n",
    "CHECK_Consolidated_links = []\n",
    "for link in data[\"CHECK_Consolidated Link\"]:\n",
    "    CHECK_Consolidated_links.append(scrape_CHECK_Consolidated_links(link) if pd.notnull(link) else \"\")\n",
    "    print(count)\n",
    "    count = count+1\n",
    "\n",
    "# Add the scraped links to the DataFrame\n",
    "data[\"CHECK_Consolidated Link_2\"] = CHECK_Consolidated_links\n",
    "\n",
    "# Save DataFrame back to CSV\n",
    "data.to_csv(\"CHECK_Consolidated Link_2.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "major",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
